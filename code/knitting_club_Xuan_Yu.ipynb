{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 15s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Import the packages needed for classification\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Load the VGG model\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "img_rows, img_cols, img_channel = 224, 224, 3\n",
    "\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, img_channel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInclude the functions used for loading, preprocessing, features extraction, \\nclassification, and performance evaluation\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Set directory parameters\n",
    "'''\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "dir_train_images  = 'data/training/'\n",
    "dir_test_images   = 'data/testing/'\n",
    "dir_train_labels  = 'data/labels_training.csv'\n",
    "dir_test_ids      = 'data/sample_submission.csv'\n",
    "\n",
    "'''\n",
    "Include the functions used for loading, preprocessing, features extraction, \n",
    "classification, and performance evaluation\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_data, dir_labels, training=True):\n",
    "    ''' Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_extract_features(data):\n",
    "    '''Preprocess data and extract features\n",
    "    \n",
    "    Preprocess: normalize, scale, repair\n",
    "    Extract features: transformations and dimensionality reduction\n",
    "    '''\n",
    "    # Here, we do something trivially simple: we take the average of the RGB\n",
    "    # values to produce a grey image, transform that into a vector, then\n",
    "    # extract the mean and standard deviation as features.\n",
    "    \n",
    "    # Make the image grayscale\n",
    "    data = np.mean(data, axis=3)\n",
    "    \n",
    "    # Vectorize the grayscale matrices\n",
    "    vectorized_data = data.reshape(data.shape[0],-1)\n",
    "    \n",
    "    # extract the mean and standard deviation of each sample as features\n",
    "    feature_mean = np.mean(vectorized_data,axis=1)\n",
    "    feature_std  = np.std(vectorized_data,axis=1)\n",
    "    \n",
    "    # Combine the extracted features into a single feature vector\n",
    "    features = np.stack((feature_mean,feature_std),axis=-1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_classifier():\n",
    "    '''Shared function to select the classifier for both performance evaluation\n",
    "    and testing\n",
    "    '''\n",
    "    return KNeighborsClassifier(n_neighbors=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_classifier_rf(estimator):\n",
    "    return RandomForestClassifier(n_estimators=estimator, n_jobs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_performance_assessment(X,y,k,clf):\n",
    "    '''Cross validated performance assessment\n",
    "    \n",
    "    X   = training data\n",
    "    y   = training labels\n",
    "    k   = number of folds for cross validation\n",
    "    clf = classifier to use\n",
    "    \n",
    "    Divide the training data into k folds of training and validation data. \n",
    "    For each fold the classifier will be trained on the training data and\n",
    "    tested on the validation data. The classifier prediction scores are \n",
    "    aggregated and output\n",
    "    '''\n",
    "    # Establish the k folds\n",
    "    prediction_scores = np.empty(y.shape[0],dtype='object')\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        # Extract the training and validation data for this fold\n",
    "        X_train, X_val   = X[train_index], X[val_index]\n",
    "        y_train          = y[train_index]\n",
    "        \n",
    "        # Train the classifier\n",
    "        X_train_features = preprocess_and_extract_features(X_train)\n",
    "        clf              = clf.fit(X_train_features,y_train)\n",
    "        \n",
    "        # Test the classifier on the validation data for this fold\n",
    "        X_val_features   = preprocess_and_extract_features(X_val)\n",
    "        cpred            = clf.predict_proba(X_val_features)\n",
    "        \n",
    "        # Save the predictions for this fold\n",
    "        prediction_scores[val_index] = cpred[:,1]\n",
    "    return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, prediction_scores):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, prediction_scores, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = 'AUC = {:0.3f}'.format(auc)\n",
    "   \n",
    "    plt.plot([0,1],[0,1],'--', color='gray', label='Chance')\n",
    "    plt.plot(fpr, tpr, label=legend_string)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid('on')\n",
    "    plt.axis('square')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "def load_data_cnn(dir_data, dir_labels, training=True):\n",
    "    ''' Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = load_img(fname, target_size=(224, 224))\n",
    "        data.append(img_to_array(image))\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Sample script for cross validated performance\n",
    "'''\n",
    "# Set parameters for the analysis\n",
    "num_training_folds = 20\n",
    "\n",
    "# Load the data\n",
    "\n",
    "data, labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "plt.imshow(data[0])\n",
    "plt.show()\n",
    "\n",
    "# Choose which classifier to use\n",
    "# clf = set_classifier()\n",
    "\n",
    "\n",
    "# Perform cross validated performance assessment\n",
    "# max_auc, best = 0.5, 0\n",
    "# for estimator in range(50,1000,50):\n",
    "#     clf = set_classifier_rf(estimator)\n",
    "#     prediction_scores = cv_performance_assessment(data,labels,num_training_folds,clf)\n",
    "#     auc = metrics.roc_auc_score(labels, prediction_scores)\n",
    "#     if auc > max_auc:\n",
    "#         best = estimator\n",
    "#         max_auc = auc\n",
    "# print(best)\n",
    "\n",
    "# clf = set_classifier_rf(800)\n",
    "# prediction_scores = cv_performance_assessment(data,labels,num_training_folds,clf)\n",
    "# Compute and plot the ROC curves\n",
    "# plot_roc(labels, prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_data_cnn(dir_train_images, dir_train_labels, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (1500, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print('data', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (1200, 224, 224, 3)\n",
      "y_train (1200,)\n",
      "x_test (300, 224, 224, 3)\n",
      "y_test (300,)\n"
     ]
    }
   ],
   "source": [
    "val_split_num = int(round(0.2*len(labels)))\n",
    "x_train = data[val_split_num:]\n",
    "y_train = labels[val_split_num:]\n",
    "x_test = data[:val_split_num]\n",
    "y_test = labels[:val_split_num]\n",
    "\n",
    "print('x_train', x_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('x_test', x_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 6423041   \n",
      "=================================================================\n",
      "Total params: 21,137,729\n",
      "Trainable params: 21,137,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "add_model = Sequential()\n",
    "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "add_model.add(Dense(256, activation='relu'))\n",
    "add_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 724s 20s/step - loss: 0.5200 - acc: 0.7492 - val_loss: 0.5150 - val_acc: 0.7433\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 717s 19s/step - loss: 0.4581 - acc: 0.7871 - val_loss: 0.4219 - val_acc: 0.8267\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 755s 20s/step - loss: 0.4377 - acc: 0.8083 - val_loss: 0.3701 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 778s 21s/step - loss: 0.3979 - acc: 0.8184 - val_loss: 0.3355 - val_acc: 0.8633\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 782s 21s/step - loss: 0.3567 - acc: 0.8496 - val_loss: 0.3633 - val_acc: 0.8200\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 752s 20s/step - loss: 0.3315 - acc: 0.8615 - val_loss: 0.2706 - val_acc: 0.9067\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 698s 19s/step - loss: 0.3019 - acc: 0.8792 - val_loss: 0.2463 - val_acc: 0.9200\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 705s 19s/step - loss: 0.3048 - acc: 0.8657 - val_loss: 0.3504 - val_acc: 0.8367\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 725s 20s/step - loss: 0.2579 - acc: 0.8919 - val_loss: 0.2083 - val_acc: 0.9467\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 733s 20s/step - loss: 0.2270 - acc: 0.9088 - val_loss: 0.2413 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=30, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1, \n",
    "        horizontal_flip=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc', save_best_only=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation CNN\n",
    "\n",
    "# prediction_scores = cv_performance_assessment(data,labels,num_training_folds,model)\n",
    "# plot_roc(labels, prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample script for producing a Kaggle submission\n",
    "'''\n",
    "\n",
    "produce_submission_cnn = True # Switch this to True when you're ready to create a submission for Kaggle\n",
    "\n",
    "if produce_submission_cnn:\n",
    "    # Load the test data and test the classifier\n",
    "    test_data, ids = load_data_cnn(dir_test_images, dir_test_ids, training=False)\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data /= 255\n",
    "    # print('test_data', test_data.shape)\n",
    "    predictions = model.predict(test_data)\n",
    "    test_scores  = [val for sublist in predictions for val in sublist]\n",
    "\n",
    "    # Save the predictions to a CSV file for upload to Kaggle\n",
    "    submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores})\n",
    "    submission_file.to_csv('submission_cnn.csv',\n",
    "                           columns=['id','score'],\n",
    "                           index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Sample script for producing a Kaggle submission\n",
    "'''\n",
    "\n",
    "produce_submission = False # Switch this to True when you're ready to create a submission for Kaggle\n",
    "\n",
    "if produce_submission:\n",
    "    # Load data, extract features, and train the classifier on the training data\n",
    "    training_data, training_labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "    training_features              = preprocess_and_extract_features(training_data)\n",
    "    clf                            = set_classifier_rf(800)\n",
    "    clf.fit(training_features,training_labels)\n",
    "\n",
    "    # Load the test data and test the classifier\n",
    "    test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "    test_features  = preprocess_and_extract_features(test_data)\n",
    "    test_scores    = clf.predict_proba(test_features)[:,1]\n",
    "\n",
    "    # Save the predictions to a CSV file for upload to Kaggle\n",
    "    submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores})\n",
    "    submission_file.to_csv('submission_rf.csv',\n",
    "                           columns=['id','score'],\n",
    "                           index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38125 0.0625  0.33625 0.75875 0.32375 0.31    0.505   0.5975  0.25\n",
      " 0.5525  0.03125 0.20625 0.1425  0.35375 0.20875 0.      0.07375 0.18375\n",
      " 0.1475  0.69875 0.19875 0.33125 0.07625 0.27875 0.49875 0.00625 0.10875\n",
      " 0.03125 0.1175  0.0625  0.24875 0.4425  0.20625 0.10625 0.14375 0.4725\n",
      " 0.3     0.0325  0.12875 0.13125 0.4875  0.      0.27875 0.41875 0.615\n",
      " 0.49125 0.24    0.5525  0.50375 0.7325  0.3875  0.9     0.4525  0.6425\n",
      " 0.16625 0.17875 0.2325  0.00125 0.27    0.47375 0.115   0.85375 0.24125\n",
      " 0.34625 0.29    0.86125 0.64375 0.2475  0.48875 0.63    0.135   0.0575\n",
      " 0.52875 0.245   0.56125 0.525   0.48125 0.39375 0.52625 0.31375 0.085\n",
      " 0.025   0.69    0.47375 0.73    0.24625 0.35625 0.7575  0.47125 0.33875\n",
      " 0.0775  0.2125  0.16875 0.57375 0.42125 0.185   0.86375 0.24125 0.36\n",
      " 0.65125 0.14875 0.28375 0.3275  0.05    0.315   0.25375 0.405   0.0925\n",
      " 0.265   0.005   0.5325  0.02875 0.46125 0.09875 0.12125 0.41125 0.22\n",
      " 0.73625 0.26    0.23625 0.52125 0.07375 0.12625 0.38875 0.28875 0.3225\n",
      " 0.005   0.5025  0.09875 0.1375  0.51375 0.3975  0.06125 0.04625 0.1625\n",
      " 0.2125  0.6525  0.49    0.09875 0.31125 0.13375 0.15375 0.005   0.455\n",
      " 0.42125 0.56    0.08125 0.0025  0.08625 0.26    0.33875 0.24875 0.56\n",
      " 0.72125 0.08625 0.62375 0.67625 0.66375 0.50875 0.07625 0.28375 0.21375\n",
      " 0.19125 0.235   0.23375 0.31    0.23375 0.34375 0.6975  0.1     0.36625\n",
      " 0.4825  0.77125 0.18625 0.34375 0.26625 0.3375  0.155   0.235   0.1275\n",
      " 0.00375 0.51    0.68875 0.255   0.715   0.05375 0.2     0.3175  0.01875\n",
      " 0.065   0.05    0.44125 0.50125 0.09    0.06375 0.54375 0.13875 0.38875\n",
      " 0.74    0.2225  0.315   0.16    0.3     0.22    0.29125 0.18    0.405\n",
      " 0.265   0.02    0.77875 0.77375 0.03875 0.75125 0.39375 0.22375 0.00125\n",
      " 0.4575  0.59    0.335   0.56    0.285   0.5025  0.13125 0.225   0.0875\n",
      " 0.21    0.365   0.4425  0.00375 0.5025  0.52    0.40125 0.24625 0.715\n",
      " 0.13875 0.38    0.155   0.63875 0.36625 0.34875 0.085   0.6525  0.05125\n",
      " 0.      0.54    0.17375 0.14125 0.53875 0.48375 0.14625 0.0325  0.6425\n",
      " 0.04625 0.4     0.2475  0.47625 0.2325  0.27    0.30625 0.4525  0.435\n",
      " 0.02625 0.7075  0.78375 0.535   0.2     0.39625 0.30625 0.72125 0.78\n",
      " 0.4375  0.32375 0.0075  0.12375 0.6325  0.27375 0.275   0.42625 0.3925\n",
      " 0.3025  0.46125 0.41625 0.37625 0.22375 0.19125 0.0425  0.82625 0.1975\n",
      " 0.03875 0.65375 0.55    0.23    0.19    0.0925  0.345   0.08875 0.53875\n",
      " 0.34125 0.6325  0.155   0.3     0.39    0.06125 0.235   0.47875 0.58\n",
      " 0.29625 0.3625  0.4     0.6475  0.66375 0.21    0.47125 0.05625 0.18875\n",
      " 0.2675  0.275   0.0525  0.1625  0.175   0.95875 0.31    0.      0.4225\n",
      " 0.395   0.05875 0.41875 0.47625 0.75    0.61    0.23625 0.0625  0.335\n",
      " 0.48    0.47375 0.54125 0.61875 0.47625 0.1525  0.71375 0.00125 0.61875\n",
      " 0.21875 0.35375 0.14375 0.19125 0.1     0.0275  0.26625 0.26125 0.52875\n",
      " 0.14875 0.0225  0.175   0.11125 0.1775  0.0675  0.02    0.70125 0.14625\n",
      " 0.26375 0.74375 0.81    0.55125 0.4425  0.23875 0.385   0.02    0.08625\n",
      " 0.57125 0.575   0.30125 0.28375 0.19375 0.05375 0.09875 0.465   0.425\n",
      " 0.00375 0.045   0.0325  0.6725  0.39    0.205   0.27625 0.10125 0.8425\n",
      " 0.39875 0.64875 0.21125 0.73125 0.32375 0.185   0.165   0.85    0.0125\n",
      " 0.2725  0.66    0.35    0.275   0.60625 0.62    0.1625  0.3175  0.02125\n",
      " 0.0775  0.21125 0.05625 0.28625 0.23375 0.0725  0.10375 0.53875 0.44625\n",
      " 0.13375 0.3375  0.5575  0.7925  0.53    0.365   0.26625 0.66    0.01\n",
      " 0.16625 0.36125 0.56125 0.13375 0.33375 0.38125 0.66125 0.15125 0.42875\n",
      " 0.12625 0.0875  0.15    0.2525  0.6575  0.23625 0.025   0.0125  0.00625\n",
      " 0.6975  0.04125 0.1425  0.7475  0.2425  0.65375 0.1175  0.5575  0.27375\n",
      " 0.34875 0.17    0.1875  0.66625 0.00375 0.03375 0.23    0.03    0.61625\n",
      " 0.385   0.0525  0.      0.16125 0.09875 0.07625 0.17875 0.44    0.2875\n",
      " 0.01    0.14875 0.04125 0.28    0.72125 0.52375 0.3225  0.22625 0.11\n",
      " 0.285   0.04    0.68    0.00625 0.70875 0.46375 0.3175  0.      0.315\n",
      " 0.84    0.75375 0.89875 0.2275  0.22125 0.05125 0.24625 0.04875 0.2275\n",
      " 0.4175  0.4625  0.58375 0.24375 0.03    0.0175  0.43375 0.555   0.175\n",
      " 0.28625 0.55125 0.0125  0.77625 0.15125 0.56375 0.12375 0.26875 0.625\n",
      " 0.65625 0.21875 0.7275  0.2175  0.555   0.0125  0.48625 0.48625 0.1775\n",
      " 0.0925  0.77875 0.24125 0.255   0.0825  0.0525  0.3475  0.3325  0.75625\n",
      " 0.30375 0.54625 0.005   0.19375 0.315   0.16    0.4425  0.14375 0.60875\n",
      " 0.2525  0.6325  0.      0.015   0.45875 0.23125 0.5325  0.74625 0.00125\n",
      " 0.61875 0.3125  0.55625 0.60875 0.51875 0.85875 0.475   0.66    0.26625]\n"
     ]
    }
   ],
   "source": [
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
